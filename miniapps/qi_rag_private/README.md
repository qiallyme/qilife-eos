# Q Tiered RAG Stack (MVP)

This repository provides a **minimal viable product (MVP)** for a Retrieval‑Augmented Generation (RAG) system with support for multiple classification tiers.  You can ingest your personal or business documents into a vector database and query them via a chatbot while respecting classification boundaries.  Each tier can choose whether to sync to a cloud fallback node or remain local only.

The MVP is opinionated but deliberately simple: it runs locally with Docker, uses [Qdrant](https://qdrant.tech/) for embeddings storage and [FastAPI](https://fastapi.tiangolo.com/) to expose HTTP endpoints.  A folder watcher automatically ingests changed files.  You can extend it later to support user authentication, fancy UIs, or additional back‑ends.

## Features

- **Classification tiers** – Four built‑in tiers (`UNCLASS`, `CLASSIFIED`, `ULTRA`, `MEO`).  Files are routed to the appropriate Qdrant collection based on their folder name or front‑matter metadata.
- **Per‑tier policies** – Each tier declares whether it can fall back to a cloud node if the local node is unavailable.  By default `UNCLASS` and `CLASSIFIED` allow cloud fallback, while `ULTRA` and `MEO` are local‑only.
- **Local‑first retrieval** – Queries are answered from the local vector database first.  If the local node is unreachable for a given tier and fallback is allowed, the server proxies the request to a remote API defined in your `.env`.
- **Folder watcher** – A simple watcher monitors your data directories and triggers ingestion whenever files are created or modified.  It supports markdown, text and PDF files out of the box.
- **Environment driven configuration** – Configure collections, folder mapping, model names and cloud endpoint in a `.env` file.  An example file is provided.

## Getting Started

### 1. Install dependencies

```bash
git clone https://example.com/q_tier_rag.git
cd q_tier_rag

# create a virtualenv (optional)
python -m venv .venv
source .venv/bin/activate

pip install -r requirements.txt
```

Alternatively you can use Docker; see the `docker-compose.yml` at the root of the project.

### 2. Configure your environment

Copy `.env.example` to `.env` and adjust the values.  Important keys:

- `DATA_ROOT` – path to the directory containing per‑tier subfolders (e.g. `V:/data` on Windows or `/data` on Linux).  The watcher looks here.
- `QDRANT_HOST` & `QDRANT_PORT` – connection parameters for your local Qdrant instance.
- `TIER_COLLECTIONS` – mapping from tier name to Qdrant collection name.  Comma separated `tier:collection` entries.
- `FOLDER_TIERS` – mapping from folder name to tier.  For example `unclass:UNCLASS,classified:CLASSIFIED`.
- `CLOUD_ENDPOINT` – URL of your cloud fallback API; omit or leave empty if you do not have one.
- `TIER_POLICIES` – per‑tier JSON describing whether each tier permits cloud fallback.

### 3. Start Qdrant and the API server

To launch everything via Docker run:

```bash
docker compose up -d

# pull a model for Ollama if using local LLM (optional)
docker compose exec ollama ollama pull llama3.1:8b
```

If you prefer to run locally without Docker you must start Qdrant and then run:

```bash
uvicorn app.main:app --reload
```

### 4. Ingest your documents

Place files into subfolders under `DATA_ROOT` matching your tiers (e.g. `data/unclass`, `data/ultra`).  You can override the tier for a particular file by adding a YAML front‑matter block to a markdown file:

```md
---
title: Project Plan
classification: CLASSIFIED
---
This note is classified.
```

Run the ingestion script to index existing files:

```bash
python scripts/ingest.py path/to/folder
```

Or start the watcher to automatically ingest new/updated files:

```bash
python scripts/watch_folder.py
```

### 5. Ask questions

Send a POST request to the `/chat` endpoint with a `question` parameter.  Optionally include a comma‑separated list of tiers to search:

```bash
curl -X POST http://localhost:8000/chat -d "question=What is the project plan?&tiers=UNCLASS,CLASSIFIED"
```

The API will respond with an answer generated by the LLM along with context documents and their tiers.  If the local node cannot answer for some tiers and fallback is allowed, the server will call the configured `CLOUD_ENDPOINT`.

## Folder Layout

```
q_tier_rag/
├── app/
│   ├── main.py          # FastAPI application exposing /ingest and /chat endpoints
│   ├── rag.py           # Helper functions for embedding and retrieving text
│   ├── llm.py           # Abstraction to call a local LLM via Ollama or remote API
│   └── utils.py         # Classification and parsing utilities
├── scripts/
│   ├── ingest.py        # CLI script to ingest an entire directory
│   └── watch_folder.py  # Folder watcher that triggers ingestion on file changes
├── .env.example         # Example environment configuration
├── docker-compose.yml   # Bring up Qdrant, Ollama and the API server
├── requirements.txt     # Python dependencies
└── README.md            # This file
```

## Extending

- **Cloud fallback** – The current implementation supports proxying to a single remote API.  Add authentication or load balancing as needed.
- **Authentication** – Protect your endpoints using API keys, OAuth, Supabase Auth or another identity provider.  Modify `app/main.py` to enforce authentication and issue per‑tier claims.
- **Additional tiers** – Add more tiers by updating `TIER_COLLECTIONS`, `FOLDER_TIERS` and `TIER_POLICIES` in your `.env`.
- **Embedding models** – Change the `EMBEDDING_MODEL` environment variable to point at a different SentenceTransformer (e.g. `all-MiniLM-L6-v2`).
- **Chunking** – Adjust `CHUNK_SIZE` and `CHUNK_OVERLAP` in `.env` to tune how text is split prior to embedding.

## License

This MVP is provided as-is under the MIT license.  Use at your own risk and feel free to adapt it for personal or commercial use.